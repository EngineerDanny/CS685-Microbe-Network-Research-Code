{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 5\n",
    "TOP_COLS = 30\n",
    "TOP_GRPS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMPv13_otu_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project ID : 1928\n",
    "# Subject ID: SRS \n",
    "# Experiment ID: SRX\n",
    "# Run ID: SRR\n",
    "\n",
    "# df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv13_otu_table.csv').T\n",
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv35_otu_table.csv').T\n",
    "# Convert the index to a series and split it\n",
    "id_df = df.index.to_series().str.split('.', expand=True)\n",
    "# Rename the columns as per your requirement\n",
    "id_df.columns = ['Project_ID', 'SRS_ID', 'SRX_ID', 'SRR_ID']\n",
    "# Now 'id_df' has the separate columns. You can join this back to your original DataFrame if needed\n",
    "# join infront of the df\n",
    "df = df.join(id_df).reset_index(drop=True)\n",
    "df = df.sort_values('SRX_ID')\n",
    "# drop the columns that are not needed\n",
    "df = df.drop(['Project_ID', 'SRS_ID', 'SRR_ID'], axis=1)\n",
    "# rename the SRX_ID column to Sample_ID\n",
    "df = df.rename(columns={'SRX_ID': 'Group_ID'}).reset_index(drop=True)\n",
    "\n",
    "# give the group ID a number each\n",
    "df['Group_ID'] = (pd.factorize(df['Group_ID'])[0] + 1)\n",
    "\n",
    "# drop the Group_ID column\n",
    "# df = df.drop(['Group_ID'], axis=1)\n",
    "# df = np.log(df + 1)\n",
    "# df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv13_otu_table_log.csv')\n",
    "\n",
    "df['Group_ID'] = df['Group_ID'].astype(int)\n",
    "df\n",
    "\n",
    "# df_selected is without the Group_ID column\n",
    "df_selected = df.drop(['Group_ID'], axis=1) \n",
    "df_selected = np.log10(df_selected + 1)\n",
    "df_selected['Group_ID'] = df['Group_ID']\n",
    "df_selected\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv13_sub_log_update.csv', index=False)\n",
    "equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv35_sub_log_update.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns: 10730\n",
      "Columns after zero threshold: 35\n",
      "Final number of columns: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>879972</th>\n",
       "      <th>341460</th>\n",
       "      <th>858896</th>\n",
       "      <th>561636</th>\n",
       "      <th>967427</th>\n",
       "      <th>949789</th>\n",
       "      <th>98605</th>\n",
       "      <th>4309323</th>\n",
       "      <th>4438988</th>\n",
       "      <th>4422456</th>\n",
       "      <th>...</th>\n",
       "      <th>4346977</th>\n",
       "      <th>4447394</th>\n",
       "      <th>4374753</th>\n",
       "      <th>3864823</th>\n",
       "      <th>1082539</th>\n",
       "      <th>12574</th>\n",
       "      <th>1089121</th>\n",
       "      <th>3801267</th>\n",
       "      <th>4405869</th>\n",
       "      <th>Group_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.309630</td>\n",
       "      <td>2.710963</td>\n",
       "      <td>1.903090</td>\n",
       "      <td>1.447158</td>\n",
       "      <td>1.643453</td>\n",
       "      <td>1.875061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>1.579784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3.031408</td>\n",
       "      <td>1.255273</td>\n",
       "      <td>2.033424</td>\n",
       "      <td>1.612784</td>\n",
       "      <td>1.431364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2.252853</td>\n",
       "      <td>2.675778</td>\n",
       "      <td>1.643453</td>\n",
       "      <td>1.230449</td>\n",
       "      <td>1.690196</td>\n",
       "      <td>1.724276</td>\n",
       "      <td>2.086360</td>\n",
       "      <td>1.949390</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>1.875061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>2.518514</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>2.025306</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>2.113943</td>\n",
       "      <td>2.235528</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>1.556303</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>1.819544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.991226</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>2.227887</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>3.595165</td>\n",
       "      <td>2.225309</td>\n",
       "      <td>2.599883</td>\n",
       "      <td>2.139879</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>...</td>\n",
       "      <td>1.748188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.093422</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>3.161068</td>\n",
       "      <td>2.633468</td>\n",
       "      <td>2.610660</td>\n",
       "      <td>1.707570</td>\n",
       "      <td>2.267172</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>1.361728</td>\n",
       "      <td>1.732394</td>\n",
       "      <td>2.068186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>1.913814</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>1.491362</td>\n",
       "      <td>2.123852</td>\n",
       "      <td>2.294466</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>3.296007</td>\n",
       "      <td>2.600973</td>\n",
       "      <td>2.702431</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>2.418301</td>\n",
       "      <td>2.220108</td>\n",
       "      <td>1.531479</td>\n",
       "      <td>1.770852</td>\n",
       "      <td>1.732394</td>\n",
       "      <td>2.230449</td>\n",
       "      <td>...</td>\n",
       "      <td>1.204120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.556303</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.806180</td>\n",
       "      <td>2.610660</td>\n",
       "      <td>2.518514</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>1.662758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.725422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        879972    341460    858896    561636    967427    949789     98605  \\\n",
       "175   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "176   2.309630  2.710963  1.903090  1.447158  1.643453  1.875061  0.000000   \n",
       "177   3.031408  1.255273  2.033424  1.612784  1.431364  0.000000  0.301030   \n",
       "178   2.252853  2.675778  1.643453  1.230449  1.690196  1.724276  2.086360   \n",
       "179   2.113943  2.235528  0.778151  0.301030  0.845098  0.698970  0.845098   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1885  3.595165  2.225309  2.599883  2.139879  0.602060  0.477121  0.301030   \n",
       "1886  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1887  3.161068  2.633468  2.610660  1.707570  2.267172  2.107210  1.204120   \n",
       "1888  3.296007  2.600973  2.702431  1.924279  2.418301  2.220108  1.531479   \n",
       "1889  1.662758  0.000000  0.698970  0.000000  0.000000  0.477121  0.903090   \n",
       "\n",
       "       4309323   4438988   4422456  ...   4346977   4447394   4374753  \\\n",
       "175   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "176   0.301030  0.477121  1.579784  ...  0.602060  0.301030  0.000000   \n",
       "177   0.000000  0.301030  0.301030  ...  0.698970  0.000000  0.000000   \n",
       "178   1.949390  0.301030  1.875061  ...  0.301030  0.698970  2.518514   \n",
       "179   1.556303  0.301030  1.819544  ...  0.000000  0.000000  1.991226   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1885  0.477121  0.301030  0.602060  ...  1.748188  0.000000  1.204120   \n",
       "1886  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1887  1.361728  1.732394  2.068186  ...  0.845098  0.845098  1.913814   \n",
       "1888  1.770852  1.732394  2.230449  ...  1.204120  0.000000  1.556303   \n",
       "1889  0.301030  0.000000  0.000000  ...  0.000000  3.725422  0.000000   \n",
       "\n",
       "       3864823   1082539     12574   1089121   3801267   4405869  Group_ID  \n",
       "175   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000         6  \n",
       "176   0.000000  0.000000  0.301030  1.113943  1.924279  0.000000         6  \n",
       "177   0.000000  0.845098  0.000000  0.000000  0.000000  0.000000         6  \n",
       "178   1.477121  1.146128  0.000000  0.698970  2.025306  0.477121         6  \n",
       "179   0.778151  0.000000  0.778151  0.477121  2.227887  0.698970         6  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1885  0.301030  0.000000  0.000000  1.301030  0.000000  2.093422       130  \n",
       "1886  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000       130  \n",
       "1887  0.602060  0.602060  1.491362  2.123852  2.294466  0.301030       130  \n",
       "1888  0.845098  1.000000  1.806180  2.610660  2.518514  0.602060       130  \n",
       "1889  0.000000  0.301030  0.000000  0.000000  0.000000  0.698970       130  \n",
       "\n",
       "[700 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read and transform the data\n",
    "# df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv13_otu_table.csv').T\n",
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv35_otu_table.csv').T\n",
    "\n",
    "# Process IDs\n",
    "id_df = df.index.to_series().str.split('.', expand=True)\n",
    "id_df.columns = ['Project_ID', 'SRS_ID', 'SRX_ID', 'SRR_ID']\n",
    "df = df.join(id_df).reset_index(drop=True)\n",
    "df = df.sort_values('SRX_ID')\n",
    "df = df.drop(['Project_ID', 'SRS_ID', 'SRR_ID'], axis=1)\n",
    "df = df.rename(columns={'SRX_ID': 'Group_ID'}).reset_index(drop=True)\n",
    "df['Group_ID'] = (pd.factorize(df['Group_ID'])[0] + 1)\n",
    "\n",
    "# Convert Group_ID to int and filter by TOP_GRPS\n",
    "df['Group_ID'] = df['Group_ID'].astype(int)\n",
    "\n",
    "\n",
    "def filter_informative_columns(df, zero_threshold=0.6, n_top_columns=30):\n",
    "    \"\"\"\n",
    "    Filter columns based on:\n",
    "    1. Percentage of zero entries (remove if too many zeros)\n",
    "    2. Sort by non-zero entries and take top columns\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - zero_threshold: maximum allowed fraction of zeros (default 0.8)\n",
    "    - n_top_columns: number of top columns to select (default 20)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with filtered columns\n",
    "    \"\"\"\n",
    "    # Separate Group_ID\n",
    "    group_id = df['Group_ID']\n",
    "    data_cols = df.drop('Group_ID', axis=1)\n",
    "    \n",
    "    # Calculate fraction of zeros in each column\n",
    "    zero_fractions = (data_cols == 0).mean()\n",
    "    \n",
    "    # Get columns that pass the zero threshold\n",
    "    mask = (zero_fractions < zero_threshold)\n",
    "    \n",
    "    # Sort columns by non-zero fraction and take top n\n",
    "    scores = (1 - zero_fractions[mask])\n",
    "    top_cols = scores.sort_values(ascending=False).head(n_top_columns).index\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    df_filtered = data_cols[top_cols]\n",
    "    df_filtered = np.log10(df_filtered + 1)\n",
    "    df_filtered['Group_ID'] = group_id\n",
    "    \n",
    "    # Print some information about the filtering\n",
    "    print(f\"Original number of columns: {len(data_cols.columns)}\")\n",
    "    print(f\"Columns after zero threshold: {len(scores)}\")\n",
    "    print(f\"Final number of columns: {len(top_cols)}\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Apply the filtering\n",
    "df_selected = filter_informative_columns(df)\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "\n",
    "# Calculate group sparsity and select top groups based on least sparse groups\n",
    "group_sparsity = equal_samples_df.groupby('Group_ID').apply(\n",
    "    lambda x: (x.drop('Group_ID', axis=1) == 0).mean().mean()\n",
    ").sort_values()  # Sort groups by their sparsity\n",
    "\n",
    "# Take the top N least sparse groups\n",
    "top_groups = group_sparsity.head(TOP_GRPS).index\n",
    "equal_samples_df = equal_samples_df[equal_samples_df['Group_ID'].isin(top_groups)]\n",
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/HMPv35_11_15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovingPictures_otu_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/MovingPictures_otu_table.csv').T\n",
    "id_df = df.index.to_series().str.split('.', expand=True)\n",
    "id_df.columns = ['Project_ID', 'Sample_ID', 'S_Constant', 'Group_ID', 'Sequence_Keyword']\n",
    "df = df.join(id_df).reset_index(drop=True)\n",
    "df = df.sort_values('Group_ID')\n",
    "df = df.drop(['Project_ID', 'Sample_ID', 'S_Constant', 'Sequence_Keyword'], axis=1).reset_index(drop=True)\n",
    "\n",
    "# drop the Group_ID column\n",
    "# df = df.drop(['Group_ID'], axis=1)\n",
    "# df = np.log(df + 1)\n",
    "# df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/MovingPictures_otu_table_log.csv')\n",
    "# df\n",
    "\n",
    "df['Group_ID'] = (pd.factorize(df['Group_ID'])[0] + 1)\n",
    "df['Group_ID'] = df['Group_ID'].astype(int)\n",
    "\n",
    "# Apply the filtering\n",
    "df_selected = filter_informative_columns(df)\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "\n",
    "# Calculate group sparsity and select top groups based on least sparse groups\n",
    "group_sparsity = equal_samples_df.groupby('Group_ID').apply(\n",
    "    lambda x: (x.drop('Group_ID', axis=1) == 0).mean().mean()\n",
    ").sort_values()  # Sort groups by their sparsity\n",
    "\n",
    "# Take the top N least sparse groups\n",
    "top_groups = group_sparsity.head(TOP_GRPS).index\n",
    "equal_samples_df = equal_samples_df[equal_samples_df['Group_ID'].isin(top_groups)]\n",
    "equal_samples_df\n",
    "\n",
    "'''\n",
    "# df_selected is without the Group_ID column\n",
    "df_selected = df.drop(['Group_ID'], axis=1) \n",
    "df_selected = np.log10(df_selected + 1)\n",
    "df_selected['Group_ID'] = df['Group_ID']\n",
    "df_selected\n",
    "\n",
    "# column_sums = df.sum(axis=0).sort_values(ascending=False)\n",
    "# top_20_columns = column_sums.head(TOP_COLS).index\n",
    "# df_selected = df[top_20_columns]\n",
    "# df_selected = np.log10(df_selected + 1)\n",
    "# df_selected['Group_ID'] = df['Group_ID']\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "equal_samples_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/MovingPictures_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## qa10394_otu_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/qa10394_otu_table.csv').T\n",
    "id_df = df.index.to_series().str.split('.', expand=True)\n",
    "# use only column with index 1\n",
    "id_df = id_df.iloc[:, 1]\n",
    "id_df = id_df.rename('Group_ID')\n",
    "df = df.join(id_df).reset_index(drop=True)\n",
    "df = df.sort_values('Group_ID')\n",
    "df = df[(df.Group_ID != \"BLANK\") & (df.Group_ID != \"mistake\")]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# drop the Group_ID column\n",
    "#df = df.drop(['Group_ID'], axis=1)\n",
    "#df = np.log(df + 1)\n",
    "#df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/qa10394_otu_table_log.csv')\n",
    "\n",
    "df['Group_ID'] = (pd.factorize(df['Group_ID'])[0] + 1)\n",
    "df['Group_ID'] = df['Group_ID'].astype(int)\n",
    "\n",
    "# Apply the filtering\n",
    "df_selected = filter_informative_columns(df)\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "\n",
    "# Calculate group sparsity and select top groups based on least sparse groups\n",
    "group_sparsity = equal_samples_df.groupby('Group_ID').apply(\n",
    "    lambda x: (x.drop('Group_ID', axis=1) == 0).mean().mean()\n",
    ").sort_values()  # Sort groups by their sparsity\n",
    "\n",
    "# Take the top N least sparse groups\n",
    "top_groups = group_sparsity.head(TOP_GRPS).index\n",
    "equal_samples_df = equal_samples_df[equal_samples_df['Group_ID'].isin(top_groups)]\n",
    "equal_samples_df\n",
    "\n",
    "\"\"\"\n",
    "# df_selected is without the Group_ID column\n",
    "df_selected = df.drop(['Group_ID'], axis=1) \n",
    "df_selected = np.log10(df_selected + 1)\n",
    "df_selected['Group_ID'] = df['Group_ID']\n",
    "df_selected\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "equal_samples_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/qa10394_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TwinsUK_otu_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns: 8480\n",
      "Columns after zero threshold: 948\n",
      "Final number of columns: 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4463892</th>\n",
       "      <th>289734</th>\n",
       "      <th>294606</th>\n",
       "      <th>303274</th>\n",
       "      <th>4381553</th>\n",
       "      <th>287445</th>\n",
       "      <th>4447072</th>\n",
       "      <th>3327894</th>\n",
       "      <th>311820</th>\n",
       "      <th>146672</th>\n",
       "      <th>...</th>\n",
       "      <th>301910</th>\n",
       "      <th>287608</th>\n",
       "      <th>329668</th>\n",
       "      <th>337403</th>\n",
       "      <th>4449054</th>\n",
       "      <th>308907</th>\n",
       "      <th>339087</th>\n",
       "      <th>317135</th>\n",
       "      <th>4425214</th>\n",
       "      <th>Group_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2.797268</td>\n",
       "      <td>2.521138</td>\n",
       "      <td>2.962843</td>\n",
       "      <td>2.622214</td>\n",
       "      <td>2.487138</td>\n",
       "      <td>1.857332</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>3.276232</td>\n",
       "      <td>3.295567</td>\n",
       "      <td>2.068186</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079181</td>\n",
       "      <td>2.585461</td>\n",
       "      <td>2.648360</td>\n",
       "      <td>4.105817</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>2.940018</td>\n",
       "      <td>2.195900</td>\n",
       "      <td>1.869232</td>\n",
       "      <td>2.103804</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1.602060</td>\n",
       "      <td>1.785330</td>\n",
       "      <td>2.178977</td>\n",
       "      <td>2.419956</td>\n",
       "      <td>4.035190</td>\n",
       "      <td>1.785330</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>3.669782</td>\n",
       "      <td>4.045245</td>\n",
       "      <td>2.060698</td>\n",
       "      <td>...</td>\n",
       "      <td>1.662758</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>2.806858</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>2.348305</td>\n",
       "      <td>2.989450</td>\n",
       "      <td>3.068928</td>\n",
       "      <td>2.287802</td>\n",
       "      <td>2.705864</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.954243</td>\n",
       "      <td>3.288696</td>\n",
       "      <td>3.130012</td>\n",
       "      <td>3.885022</td>\n",
       "      <td>1.322219</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>3.396722</td>\n",
       "      <td>2.004321</td>\n",
       "      <td>3.622421</td>\n",
       "      <td>3.379487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778151</td>\n",
       "      <td>3.614897</td>\n",
       "      <td>0.845098</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>3.115278</td>\n",
       "      <td>3.497206</td>\n",
       "      <td>2.824126</td>\n",
       "      <td>3.153815</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>3.571942</td>\n",
       "      <td>2.369216</td>\n",
       "      <td>1.462398</td>\n",
       "      <td>2.588832</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>1.897627</td>\n",
       "      <td>3.673021</td>\n",
       "      <td>2.089905</td>\n",
       "      <td>2.238046</td>\n",
       "      <td>1.924279</td>\n",
       "      <td>...</td>\n",
       "      <td>1.832509</td>\n",
       "      <td>2.775246</td>\n",
       "      <td>2.494155</td>\n",
       "      <td>3.801404</td>\n",
       "      <td>2.060698</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>2.367356</td>\n",
       "      <td>1.602060</td>\n",
       "      <td>0.903090</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3.626443</td>\n",
       "      <td>3.052694</td>\n",
       "      <td>3.166134</td>\n",
       "      <td>0.301030</td>\n",
       "      <td>1.113943</td>\n",
       "      <td>3.658107</td>\n",
       "      <td>2.133539</td>\n",
       "      <td>2.739572</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>2.686636</td>\n",
       "      <td>...</td>\n",
       "      <td>3.235781</td>\n",
       "      <td>3.519959</td>\n",
       "      <td>3.205746</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>2.456366</td>\n",
       "      <td>1.176091</td>\n",
       "      <td>2.718502</td>\n",
       "      <td>1.491362</td>\n",
       "      <td>3.388456</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2.983626</td>\n",
       "      <td>3.082426</td>\n",
       "      <td>2.726727</td>\n",
       "      <td>3.427324</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>3.054613</td>\n",
       "      <td>3.261976</td>\n",
       "      <td>2.806858</td>\n",
       "      <td>3.285332</td>\n",
       "      <td>2.777427</td>\n",
       "      <td>...</td>\n",
       "      <td>2.547775</td>\n",
       "      <td>1.278754</td>\n",
       "      <td>3.215638</td>\n",
       "      <td>0.698970</td>\n",
       "      <td>2.708421</td>\n",
       "      <td>1.863323</td>\n",
       "      <td>2.712650</td>\n",
       "      <td>1.716003</td>\n",
       "      <td>1.041393</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2.913814</td>\n",
       "      <td>3.949195</td>\n",
       "      <td>2.875061</td>\n",
       "      <td>2.812245</td>\n",
       "      <td>2.958564</td>\n",
       "      <td>2.662758</td>\n",
       "      <td>2.397940</td>\n",
       "      <td>2.525045</td>\n",
       "      <td>3.006038</td>\n",
       "      <td>3.669224</td>\n",
       "      <td>...</td>\n",
       "      <td>2.546543</td>\n",
       "      <td>1.812913</td>\n",
       "      <td>2.431364</td>\n",
       "      <td>3.525045</td>\n",
       "      <td>2.823474</td>\n",
       "      <td>2.521138</td>\n",
       "      <td>2.100371</td>\n",
       "      <td>1.939519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>3.804071</td>\n",
       "      <td>2.950851</td>\n",
       "      <td>2.155336</td>\n",
       "      <td>3.044540</td>\n",
       "      <td>2.285557</td>\n",
       "      <td>2.699838</td>\n",
       "      <td>3.187521</td>\n",
       "      <td>2.563481</td>\n",
       "      <td>3.188366</td>\n",
       "      <td>2.911690</td>\n",
       "      <td>...</td>\n",
       "      <td>2.167317</td>\n",
       "      <td>2.247973</td>\n",
       "      <td>2.418301</td>\n",
       "      <td>0.954243</td>\n",
       "      <td>2.890421</td>\n",
       "      <td>2.454845</td>\n",
       "      <td>2.737987</td>\n",
       "      <td>1.681241</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2.245513</td>\n",
       "      <td>2.589950</td>\n",
       "      <td>2.571709</td>\n",
       "      <td>2.758912</td>\n",
       "      <td>3.407221</td>\n",
       "      <td>3.317227</td>\n",
       "      <td>3.122216</td>\n",
       "      <td>3.557026</td>\n",
       "      <td>3.571709</td>\n",
       "      <td>3.485011</td>\n",
       "      <td>...</td>\n",
       "      <td>2.665581</td>\n",
       "      <td>3.231470</td>\n",
       "      <td>3.386677</td>\n",
       "      <td>2.466868</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>1.944483</td>\n",
       "      <td>2.636488</td>\n",
       "      <td>1.826075</td>\n",
       "      <td>1.763428</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2.681241</td>\n",
       "      <td>2.195900</td>\n",
       "      <td>1.278754</td>\n",
       "      <td>2.195900</td>\n",
       "      <td>2.117271</td>\n",
       "      <td>1.977724</td>\n",
       "      <td>2.068186</td>\n",
       "      <td>1.146128</td>\n",
       "      <td>2.113943</td>\n",
       "      <td>1.623249</td>\n",
       "      <td>...</td>\n",
       "      <td>1.929419</td>\n",
       "      <td>1.875061</td>\n",
       "      <td>2.357935</td>\n",
       "      <td>2.475671</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>2.120574</td>\n",
       "      <td>2.096910</td>\n",
       "      <td>0.477121</td>\n",
       "      <td>0.602060</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      4463892    289734    294606    303274   4381553    287445   4447072  \\\n",
       "120  2.797268  2.521138  2.962843  2.622214  2.487138  1.857332  0.778151   \n",
       "121  1.602060  1.785330  2.178977  2.419956  4.035190  1.785330  0.954243   \n",
       "122  0.954243  3.288696  3.130012  3.885022  1.322219  0.954243  3.396722   \n",
       "123  3.571942  2.369216  1.462398  2.588832  1.113943  1.897627  3.673021   \n",
       "124  3.626443  3.052694  3.166134  0.301030  1.113943  3.658107  2.133539   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "595  2.983626  3.082426  2.726727  3.427324  0.954243  3.054613  3.261976   \n",
       "596  2.913814  3.949195  2.875061  2.812245  2.958564  2.662758  2.397940   \n",
       "597  3.804071  2.950851  2.155336  3.044540  2.285557  2.699838  3.187521   \n",
       "598  2.245513  2.589950  2.571709  2.758912  3.407221  3.317227  3.122216   \n",
       "599  2.681241  2.195900  1.278754  2.195900  2.117271  1.977724  2.068186   \n",
       "\n",
       "      3327894    311820    146672  ...    301910    287608    329668  \\\n",
       "120  3.276232  3.295567  2.068186  ...  2.079181  2.585461  2.648360   \n",
       "121  3.669782  4.045245  2.060698  ...  1.662758  0.698970  2.806858   \n",
       "122  2.004321  3.622421  3.379487  ...  0.778151  3.614897  0.845098   \n",
       "123  2.089905  2.238046  1.924279  ...  1.832509  2.775246  2.494155   \n",
       "124  2.739572  0.954243  2.686636  ...  3.235781  3.519959  3.205746   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "595  2.806858  3.285332  2.777427  ...  2.547775  1.278754  3.215638   \n",
       "596  2.525045  3.006038  3.669224  ...  2.546543  1.812913  2.431364   \n",
       "597  2.563481  3.188366  2.911690  ...  2.167317  2.247973  2.418301   \n",
       "598  3.557026  3.571709  3.485011  ...  2.665581  3.231470  3.386677   \n",
       "599  1.146128  2.113943  1.623249  ...  1.929419  1.875061  2.357935   \n",
       "\n",
       "       337403   4449054    308907    339087    317135   4425214  Group_ID  \n",
       "120  4.105817  0.477121  2.940018  2.195900  1.869232  2.103804         4  \n",
       "121  0.954243  2.348305  2.989450  3.068928  2.287802  2.705864         4  \n",
       "122  1.113943  0.903090  3.115278  3.497206  2.824126  3.153815         4  \n",
       "123  3.801404  2.060698  0.301030  2.367356  1.602060  0.903090         4  \n",
       "124  1.146128  2.456366  1.176091  2.718502  1.491362  3.388456         4  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "595  0.698970  2.708421  1.863323  2.712650  1.716003  1.041393        14  \n",
       "596  3.525045  2.823474  2.521138  2.100371  1.939519  1.000000        14  \n",
       "597  0.954243  2.890421  2.454845  2.737987  1.681241  0.602060        14  \n",
       "598  2.466868  1.944483  1.944483  2.636488  1.826075  1.763428        14  \n",
       "599  2.475671  0.602060  2.120574  2.096910  0.477121  0.602060        14  \n",
       "\n",
       "[240 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/TwinsUK_otu_table.csv').T\n",
    "id_df = df.index.to_series().str.split('.', expand=True)\n",
    "id_df = id_df.iloc[:, 1]\n",
    "id_df = id_df.rename('Group_ID')\n",
    "df = df.join(id_df).reset_index(drop=True)\n",
    "df = df.sort_values('Group_ID').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# drop the Group_ID column\n",
    "#df = df.drop(['Group_ID'], axis=1)\n",
    "#df = np.log(df + 1)\n",
    "#df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/TwinsUK_otu_table_log.csv')\n",
    "df['Group_ID'] = (pd.factorize(df['Group_ID'])[0] + 1)\n",
    "df['Group_ID'] = df['Group_ID'].astype(int)\n",
    "\n",
    "\"\"\"\n",
    "# df_selected is without the Group_ID column\n",
    "df_selected = df.drop(['Group_ID'], axis=1) \n",
    "df_selected = np.log10(df_selected + 1)\n",
    "df_selected['Group_ID'] = df['Group_ID']\n",
    "# df_selected\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "equal_samples_df\n",
    "\"\"\"\n",
    "\n",
    "# Apply the filtering\n",
    "df_selected = filter_informative_columns(df)\n",
    "\n",
    "# First, find the minimum group size\n",
    "min_group_size = int(df_selected['Group_ID'].value_counts().mean())\n",
    "\n",
    "# but min_group_size should be a multiple of the cv constant which is 5\n",
    "min_group_size = min_group_size - (min_group_size % CV)\n",
    "\n",
    "# remove all groups that are smaller than the min_group_size\n",
    "df_selected = df_selected.groupby('Group_ID').filter(lambda x: len(x) >= min_group_size)\n",
    "\n",
    "# Now, use groupby and sample to get equal-sized samples from each group\n",
    "equal_samples_df = df_selected.groupby('Group_ID').apply(lambda x: x.sample(min_group_size)).reset_index(drop=True)\n",
    "\n",
    "# arrange Group_ID in ascending order\n",
    "equal_samples_df = equal_samples_df.sort_values('Group_ID').reset_index(drop=True)\n",
    "\n",
    "# Calculate group sparsity and select top groups based on least sparse groups\n",
    "group_sparsity = equal_samples_df.groupby('Group_ID').apply(\n",
    "    lambda x: (x.drop('Group_ID', axis=1) == 0).mean().mean()\n",
    ").sort_values()  # Sort groups by their sparsity\n",
    "\n",
    "# Take the top N least sparse groups\n",
    "top_groups = group_sparsity.head(TOP_GRPS).index\n",
    "equal_samples_df = equal_samples_df[equal_samples_df['Group_ID'].isin(top_groups)]\n",
    "equal_samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_samples_df.to_csv('/projects/genomic-ml/da2343/ml_project_1/data/microbe_ds/TwinsUK_filtered.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
