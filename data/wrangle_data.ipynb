{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the csv files into one file\n",
    "# The csv files are from this current directory\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as p9\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from skbio.stats.composition import clr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset_name = 'amgut2_data'\n",
    "\n",
    "\n",
    "dataset_list = ['baxter_crc_data', 'crohns_data', 'glne007_data', 'global_patterns_data', 'esophagus_data', 'enterotype_data', 'hmp2prot_data', 'hmp216S_data', 'mixmpln_real_data', 'soilrep_data', 'ioral_data' ]\n",
    "\n",
    "dataset_list = ['amgut1_data', 'amgut2_data']\n",
    "\n",
    "for dataset_name in dataset_list:\n",
    "    dataset_df = pd.read_csv(f'./{dataset_name}_update.csv', header=0, index_col=0)\n",
    "    # dataset_df = pd.read_csv(f'./{dataset_name}_update.csv', header=0)\n",
    "    dataset_df_log = np.log(dataset_df + 1)\n",
    "    dataset_df_log = np.log(dataset_df_log + 1)\n",
    "    dataset_df_scaled = pd.DataFrame(StandardScaler().fit_transform(dataset_df_log), columns=dataset_df.columns) \n",
    "    dataset_df_scaled.to_csv(f'./{dataset_name}_log2_standard_scaled_transformed.csv', index=False)\n",
    "\n",
    "# scale the data\n",
    "# scaled_dataset_df = pd.DataFrame(StandardScaler(with_mean=False).fit_transform(dataset_df)) \n",
    "# clr transform the data\n",
    "# change the data to be a numpy array\n",
    "# dataset_np = dataset_df.to_numpy()\n",
    "# clr transform the data\n",
    "# dataset_np_clr = clr(dataset_np)\n",
    "\n",
    "# clr_dataset_df = pd.DataFrame(clr(dataset_df))\n",
    "\n",
    "# scaled_dataset_df.to_csv(f'./{dataset_name}_super_scaled.csv')\n",
    "print(dataset_df.shape)\n",
    "print(dataset_df_scaled.shape)\n",
    "print(dataset_df)\n",
    "print(dataset_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dataset_list = ['baxter_crc_data', 'crohns_data', 'glne007_data', 'global_patterns_data', 'esophagus_data', 'enterotype_data', 'hmp2prot_data', 'hmp216S_data', 'mixmpln_real_data', 'soilrep_data', 'ioral_data' ]\n",
    "# dataset_list = ['amgut1_data', 'amgut2_data']\n",
    "dataset_list = ['amgut2_data']\n",
    "\n",
    "\n",
    "for dataset_name in dataset_list:\n",
    "    dataset_df = pd.read_csv(f'./{dataset_name}_update.csv', header=0)\n",
    "    # Add 1 to all the values\n",
    "    # dataset_df = dataset_df + 1\n",
    "    # dataset_df = pd.read_csv(f'./{dataset_name}_update.csv', header=0)\n",
    "    # data_transformed = PowerTransformer().fit_transform(dataset_df)\n",
    "    # Fit transformer to data\n",
    "    # Transform data to normal distribution\n",
    "    data_transformed = StandardScaler().fit_transform(dataset_df)\n",
    "    dataset_df_scaled = pd.DataFrame(data_transformed, columns=dataset_df.columns) \n",
    "    # save the log transformed data\n",
    "    dataset_df_scaled.to_csv(f'./{dataset_name}_standard_scaled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw the histogram of the data to see if it is normally distributed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "dataset_list = ['amgut1_data_log_standard_scaled_transformed', \n",
    "                'amgut1_data_log1_standard_scaled_transformed', \n",
    "                'amgut1_data_log2_standard_scaled_transformed', \n",
    "                'amgut1_data_power_transformed', \n",
    "                'amgut1_data_box_cox_transformed', \n",
    "                'amgut1_data_update', ]\n",
    "\n",
    "for dataset_name in dataset_list:\n",
    "    print(dataset_name)\n",
    "    dataset_df = pd.read_csv(f'/home/da2343/cs685_fall22/data/{dataset_name}.csv', header=0)\n",
    "    dataset_df.hist(  bins=10, figsize=(30,35))\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Histogram of {dataset_name}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Generate example sparse matrix\n",
    "X = np.array([[1, 0, 2], [0, 3, 0], [4, 0, 5]])\n",
    "\n",
    "# Create logarithmic transformation function\n",
    "log_transform = FunctionTransformer(func=np.log1p, validate=True)\n",
    "\n",
    "# Apply logarithmic transformation to non-zero elements of sparse matrix\n",
    "X_transformed = log_transform.transform(X)\n",
    "\n",
    "# Print transformed matrix\n",
    "print(X)\n",
    "print(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amgut_data = pd.read_csv(\"./amgut1_data.csv\", header=0, index_col=0)\n",
    "\n",
    "# Prepare amgut2_data.csv\n",
    "amgut2_data = pd.read_csv('./amgut2_data.csv', header=0, index_col=0)\n",
    "# Make the columns into rows and vice versa\n",
    "amgut2_data = amgut2_data.T\n",
    "amgut2_data_scaled = pd.DataFrame(StandardScaler().fit_transform(amgut2_data)) \n",
    "amgut2_data_scaled.to_csv('./amgut2_data_scaled.csv')\n",
    "\n",
    "baxter_crc_data = pd.read_csv('./baxter_crc_data.csv', header=0, index_col=0)\n",
    "baxter_crc_data_scaled = pd.DataFrame(StandardScaler().fit_transform(baxter_crc_data)) \n",
    "baxter_crc_data_scaled.to_csv('./baxter_crc_data_scaled.csv')\n",
    "\n",
    "crohns_data = pd.read_csv('./crohns_data.csv', header=0, index_col=0)\n",
    "# Remove the last column\n",
    "crohns_data = crohns_data.iloc[:, :-1]\n",
    "crohns_data.to_csv('./crohns_data_update.csv')\n",
    "\n",
    "glne007_data = pd.read_csv('./glne007.csv', header=0, index_col=0)\n",
    "# Remove first 2 columns\n",
    "glne007_data = glne007_data.iloc[:, 3:]\n",
    "glne007_data.to_csv('./glne007_data_update.csv')\n",
    "\n",
    "hmp2prot_data = pd.read_csv('./hmp2prot_data.csv', header=0, index_col=0)\n",
    "hmp2prot_data = hmp2prot_data.T\n",
    "hmp2prot_data.to_csv('./hmp2prot_data_update.csv')\n",
    "\n",
    "hmp216S_data = pd.read_csv('./hmp216S_data.csv', header=0, index_col=0)\n",
    "hmp216S_data = hmp216S_data.T\n",
    "hmp216S_data.to_csv('./hmp216S_data_update.csv', )\n",
    "\n",
    "mixmpln_real_data = pd.read_csv('./mixmpln_real_data.csv', header=0, index_col=0)\n",
    "# Remove the first column\n",
    "mixmpln_real_data = mixmpln_real_data.iloc[:, 1:]\n",
    "mixmpln_real_data.to_csv('./mixmpln_real_data_update.csv')\n",
    "\n",
    "soilrep_data = pd.read_csv('./soilrep_data.csv', header=0, index_col=0)\n",
    "soilrep_data = soilrep_data.T\n",
    "mixmpln_real_data.to_csv('./soilrep_data_update.csv')\n",
    "\n",
    "esophagus_data = pd.read_csv('./esophagus_data.csv', header=0, index_col=0)\n",
    "esophagus_data = esophagus_data.T\n",
    "esophagus_data.to_csv('./esophagus_data_update.csv')\n",
    "\n",
    "enterotype_data = pd.read_csv('./enterotype_data.csv', header=0, index_col=0)\n",
    "enterotype_data = enterotype_data.T\n",
    "enterotype_data = enterotype_data.iloc[:, 1:]\n",
    "enterotype_data.to_csv('./enterotype_data_update.csv')\n",
    "\n",
    "global_patterns_data = pd.read_csv('./global_patterns_data.csv', header=0, index_col=0)\n",
    "global_patterns_data = global_patterns_data.T\n",
    "# global_patterns_data = global_patterns_data.iloc[:, 1:]\n",
    "global_patterns_data.to_csv('./global_patterns_data_update.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data():\n",
    "    # get all the csv files in the current directory\n",
    "    all_files = glob.glob(os.path.join(os.getcwd(), \"*.csv\"))\n",
    "    \n",
    "    for m_file in all_files:\n",
    "        file_name = m_file.split('/')[-1]\n",
    "        dataset = pd.read_csv(m_file)\n",
    "        scaled_dataset = StandardScaler().fit_transform(dataset)\n",
    "        scaled_df = pd.DataFrame(scaled_dataset, columns=dataset.columns)\n",
    "        scaled_df.to_csv('./scaled_' + file_name)\n",
    "        print(scaled_df)\n",
    "        \n",
    "scale_data()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the histogram of the data to see if it is normally distributed\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "my_data = pd.read_csv('/home/da2343/cs685_fall22/data/crohns_data_log_standard_scaled_transformed.csv', header=0, index_col=0)\n",
    "\n",
    "my_data.hist(bins=50, figsize=(20,15))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c51af742e141fb8ae370995bc6149e53fca1868e122616bc9da9e07ef681ffa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
