{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LassoCV, BayesianRidge, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import multivariate_normal\n",
    "import sys\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import interpolate\n",
    "from sklearn.covariance import GraphicalLassoCV, ledoit_wolf\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "class Featureless:\n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        test_features = X\n",
    "        test_nrow, test_ncol = test_features.shape\n",
    "        return np.repeat(self.mean, test_nrow)\n",
    "\n",
    "\n",
    "class SpearmanRankRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, threshold=0.0):\n",
    "        self.threshold = threshold\n",
    "        self.preprocessor1 = make_pipeline(\n",
    "            # FunctionTransformer(np.log1p),\n",
    "            # PowerTransformer(),\n",
    "            StandardScaler(with_std=True),\n",
    "            # FunctionTransformer(stats.boxcox),\n",
    "            # LambdaTransformer(func=lambda x: ss.rankdata(x, axis=0)),\n",
    "            # MinMaxScaler(),\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.y_train = y\n",
    "        # X_train_ranked_transf = ss.rankdata(X, axis=0)  \n",
    "        # self.y_train_ranked_transf = self.preprocessor2.fit_transform(ss.rankdata(y).reshape(-1, 1)).flatten()\n",
    "        self.y_train_ranked_transf = ss.rankdata(y)\n",
    "        X_train_ranked_transf = self.preprocessor1.fit_transform(ss.rankdata(X, axis=0),ss.rankdata(y) )\n",
    "        # X_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(X, axis=0))\n",
    "        # self.y_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(y))\n",
    "\n",
    "        slope_list = []\n",
    "        intercept_list = []\n",
    "\n",
    "        for index_col in range(X_train_ranked_transf.shape[1]):\n",
    "            X_col = X_train_ranked_transf[:, index_col]\n",
    "            calc_slope, calc_intercept = self.find_model_params(\n",
    "                X_col, self.y_train_ranked_transf)\n",
    "            slope_list.append(calc_slope)\n",
    "            intercept_list.append(calc_intercept)\n",
    "        # Find the mean of the gradients and intercepts\n",
    "        self.slope_list = slope_list\n",
    "        self.intercept_list = intercept_list\n",
    "        return self\n",
    "\n",
    "    def find_model_params(self, X_col, y_col):\n",
    "        calc_cor = np.corrcoef(X_col, y_col)[0, 1]\n",
    "        # If the correlation is greater than the threshold, then calculate the gradient and intercept\n",
    "        if abs(calc_cor) > self.threshold:\n",
    "            calc_slope = calc_cor * np.std(y_col) / np.std(X_col)\n",
    "            calc_intercept = np.mean(y_col) - calc_slope * np.mean(X_col)\n",
    "        else:\n",
    "            calc_slope = None\n",
    "            calc_intercept = None\n",
    "        return calc_slope, calc_intercept\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_y_list = []\n",
    "        X_test_ranked_transf = self.preprocessor1.fit_transform(ss.rankdata(X, axis=0))\n",
    "        # X_test_ranked_transf = ss.rankdata(X, axis=0)\n",
    "\n",
    "        for index_col in range(X_test_ranked_transf.shape[1]):\n",
    "            X_col = X_test_ranked_transf[:, index_col]\n",
    "            # use the average of the slope_list as the default slope\n",
    "            filtered_slope_list = [x for x in self.slope_list if x is not None]\n",
    "            mean_filtered_slope = np.mean(filtered_slope_list) if len(\n",
    "                filtered_slope_list) > 0 else 0\n",
    "            calc_slope = mean_filtered_slope if self.slope_list[\n",
    "                index_col] is None else self.slope_list[index_col]\n",
    "\n",
    "            filtered_intercept_list = [\n",
    "                x for x in self.intercept_list if x is not None]\n",
    "            mean_filtered_intercept = np.mean(filtered_intercept_list) if len(\n",
    "                filtered_intercept_list) > 0 else 0\n",
    "            calc_intercept = mean_filtered_intercept if self.intercept_list[\n",
    "                index_col] is None else self.intercept_list[index_col]\n",
    "\n",
    "            calc_y_ranked = calc_slope * X_col + calc_intercept\n",
    "\n",
    "            # remove duplicate values from self.y_train_ranked_transf and use indexes to remove items from self.y_train\n",
    "            y_train_ranked_transf_unique, sorted_indexes = np.unique(self.y_train_ranked_transf, return_index=True)\n",
    "            y_train_unique = self.y_train[sorted_indexes]\n",
    "\n",
    "            linear_interpolation = interpolate.interp1d(y_train_ranked_transf_unique, y_train_unique, fill_value=\"extrapolate\")\n",
    "\n",
    "            # cubic_spline_interpolation = interpolate.CubicSpline(\n",
    "            #     y_train_ranked_transf_unique, y_train_unique,  extrapolate=True)\n",
    "            calc_y = linear_interpolation(calc_y_ranked)\n",
    "\n",
    "            pred_y_list.append(calc_y)\n",
    "        # Find the mean of the predicted y values\n",
    "        pred_y = np.mean(np.array(pred_y_list), axis=0)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "\n",
    "data_set_name = \"amgut1\"\n",
    "n_of_samples = 50\n",
    "index_of_pred_col = 2\n",
    "\n",
    "# Name some string contants\n",
    "out_dir = \"/scratch/da2343/cs685fall22/data\"\n",
    "out_file = out_dir + f'/my_algos_{str(date.today())}_results.csv'\n",
    "\n",
    "dataset_dict = {\n",
    "    \"amgut1\": \"/home/da2343/cs685_fall22/data/amgut1_data_standard_scaled.csv\",\n",
    "    # \"crohns\": \"/home/da2343/cs685_fall22/data/crohns_data_update.csv\",\n",
    "    # \"baxter_crc\": \"/home/da2343/cs685_fall22/data/baxter_crc_data_power_transformed.csv\",\n",
    "    # \"amgut2\": \"/home/da2343/cs685_fall22/data/amgut2_data_log_standard_scaled_transformed.csv\"\n",
    "}\n",
    "\n",
    "dataset_path = dataset_dict[data_set_name]\n",
    "n_splits = 3\n",
    "\n",
    "# Import the csv file of the dataset\n",
    "dataset_pd = pd.read_csv(dataset_path, header=0)\n",
    "sub_data_dict = {}\n",
    "# drop only one column per every iteration to form the input matrix\n",
    "# make the column you removed the output\n",
    "# print the size of the input matrix\n",
    "output_vec = dataset_pd.iloc[:, index_of_pred_col].to_frame()\n",
    "input_mat = dataset_pd.drop(dataset_pd.columns[index_of_pred_col], axis=1)\n",
    "\n",
    "input_mat_update = input_mat.iloc[:n_of_samples].to_numpy()\n",
    "output_vec_update = output_vec.iloc[:n_of_samples].to_numpy().ravel()\n",
    "\n",
    "# rank input matrix and output vector\n",
    "# input_mat_update_ranked = ss.rankdata(input_mat_update, axis=0)\n",
    "# output_vec_update_ranked = ss.rankdata(output_vec_update)\n",
    "\n",
    "# # transform input matrix and output vector using PowerTransformer\n",
    "# input_mat_update_ranked_transformed = PowerTransformer().fit_transform(input_mat_update_ranked)\n",
    "# output_vec_update_ranked_transformed = PowerTransformer().fit_transform(output_vec_update_ranked.reshape(-1, 1)).ravel()\n",
    "\n",
    "\n",
    "# input_mat_update = np.sqrt(input_mat_update)\n",
    "# output_vec_update = np.sqrt(output_vec_update)\n",
    "\n",
    "data_tuple = (input_mat_update,\n",
    "              output_vec_update, index_of_pred_col)\n",
    "\n",
    "# Create a list of alphas for the LASSOCV to cross-validate against\n",
    "threshold_param_list = np.concatenate(\n",
    "    (np.linspace(0, 0.2, 125), np.linspace(0.21, 0.4, 21), np.arange(0.5, 1.01, 0.1)))\n",
    "threshold_param_dict = [{'threshold': [threshold]}\n",
    "                        for threshold in threshold_param_list]\n",
    "\n",
    "learner_dict = {\n",
    "    # 'MultiVariateNormalModel' : MultiVariateNormalModel(),\n",
    "    # 'GuassianGraphicalMethod' : GuassianGraphicalMethod(),\n",
    "    # 'Pearson Correlation':  MyPearsonRegressor(),\n",
    "    'SpearmanRankRegressor': SpearmanRankRegressor(),\n",
    "    'Featureless': Featureless(),\n",
    "}\n",
    "\n",
    "\n",
    "test_err_list = []\n",
    "\n",
    "(input_mat, output_vec, index_col) = data_tuple\n",
    "k_fold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "for fold_id, indices in enumerate(k_fold.split(input_mat)):\n",
    "    index_dict = dict(zip([\"train\", \"test\"], indices))\n",
    "    set_data_dict = {}\n",
    "    for set_name, index_vec in index_dict.items():\n",
    "        set_data_dict[set_name] = {\n",
    "            \"X\": input_mat[index_vec],\n",
    "            \"y\": output_vec[index_vec]\n",
    "        }\n",
    "   \n",
    "    actual_y = set_data_dict[\"test\"][\"y\"]\n",
    "    x_col = set_data_dict[\"test\"][\"X\"][:, index_col]\n",
    "    \n",
    "    # Spearman model prediction   \n",
    "    spearman =  SpearmanRankRegressor()\n",
    "    spearman.fit(**set_data_dict[\"train\"])\n",
    "    spearman_pred_y = spearman.predict(set_data_dict[\"test\"][\"X\"])\n",
    "    # Featureless model prediction\n",
    "    featureless = Featureless()\n",
    "    featureless.fit(**set_data_dict[\"train\"])\n",
    "    featureless_pred_y = featureless.predict(set_data_dict[\"test\"][\"X\"])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Graph of the pred_y for both spearman_pred_y and featureless_pred_y and actual_y on the y axis, and the x_col on the x axis\n",
    "    plt.scatter(x_col, actual_y, label=\"actual_y\")\n",
    "    plt.scatter(x_col, spearman_pred_y, label=\"spearman_pred_y\")\n",
    "    plt.scatter(x_col, featureless_pred_y, label=\"featureless_pred_y\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    test_err_list.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"spearman_test_err2\": mean_squared_error(actual_y, spearman_pred_y),\n",
    "        \"featureless_test_err2\": mean_squared_error(actual_y, featureless_pred_y),\n",
    "    })\n",
    "    \n",
    "\n",
    "test_err_df = pd.DataFrame(test_err_list)\n",
    "print(test_err_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fold_id  featureless_test_err2  spearman_test_err2  pearson_test_err2\n",
      "0        0               0.322297            0.315993           0.291849\n",
      "1        1               0.548365            0.662198           0.508963\n",
      "2        2               2.197654            2.389734           2.127140\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LassoCV, BayesianRidge, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import multivariate_normal\n",
    "import sys\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy import interpolate\n",
    "from sklearn.covariance import GraphicalLassoCV, ledoit_wolf\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "class Featureless:\n",
    "    def fit(self, X, y):\n",
    "        self.mean = np.mean(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        test_features = X\n",
    "        test_nrow, test_ncol = test_features.shape\n",
    "        return np.repeat(self.mean, test_nrow)\n",
    "\n",
    "\n",
    "class SpearmanRankRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, threshold=0.0):\n",
    "        self.threshold = threshold\n",
    "        self.preprocessor1 = make_pipeline(\n",
    "            MinMaxScaler(),\n",
    "            StandardScaler(),\n",
    "        )\n",
    "        self.preprocessor2 = make_pipeline(\n",
    "            MinMaxScaler(),\n",
    "            StandardScaler(),\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.y_train = y\n",
    "        # X_train_ranked_transf = ss.rankdata(X, axis=0)\n",
    "        self.y_train_ranked_transf = self.preprocessor2.fit_transform(\n",
    "            ss.rankdata(y).reshape(-1, 1)).flatten()\n",
    "        # self.y_train_ranked_transf = ss.rankdata(y)\n",
    "        X_train_ranked_transf = self.preprocessor1.fit_transform(\n",
    "            ss.rankdata(X, axis=0))\n",
    "        # X_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(X, axis=0))\n",
    "        # self.y_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(y))\n",
    "\n",
    "        slope_list = []\n",
    "        intercept_list = []\n",
    "\n",
    "        for index_col in range(X_train_ranked_transf.shape[1]):\n",
    "            X_col = X_train_ranked_transf[:, index_col]\n",
    "            calc_slope, calc_intercept = self.find_model_params(\n",
    "                X_col, self.y_train_ranked_transf)\n",
    "            slope_list.append(calc_slope)\n",
    "            intercept_list.append(calc_intercept)\n",
    "        # Find the mean of the gradients and intercepts\n",
    "        self.slope_list = slope_list\n",
    "        self.intercept_list = intercept_list\n",
    "        return self\n",
    "\n",
    "    def find_model_params(self, X_col, y_col):\n",
    "        calc_cor = np.corrcoef(X_col, y_col)[0, 1]\n",
    "        # If the correlation is greater than the threshold, then calculate the gradient and intercept\n",
    "        if abs(calc_cor) > self.threshold:\n",
    "            calc_slope = calc_cor * np.std(y_col) / np.std(X_col)\n",
    "            calc_intercept = np.mean(y_col) - calc_slope * np.mean(X_col)\n",
    "        else:\n",
    "            calc_slope = None\n",
    "            calc_intercept = None\n",
    "        return calc_slope, calc_intercept\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_y_list = []\n",
    "        X_test_ranked_transf = self.preprocessor1.fit_transform(\n",
    "            ss.rankdata(X, axis=0))\n",
    "        # X_test_ranked_transf = ss.rankdata(X, axis=0)\n",
    "\n",
    "        for index_col in range(X_test_ranked_transf.shape[1]):\n",
    "            X_col = X_test_ranked_transf[:, index_col]\n",
    "            # use the average of the slope_list as the default slope\n",
    "            filtered_slope_list = [x for x in self.slope_list if x is not None]\n",
    "            mean_filtered_slope = np.mean(filtered_slope_list) if len(\n",
    "                filtered_slope_list) > 0 else 0\n",
    "            calc_slope = mean_filtered_slope if self.slope_list[\n",
    "                index_col] is None else self.slope_list[index_col]\n",
    "\n",
    "            filtered_intercept_list = [\n",
    "                x for x in self.intercept_list if x is not None]\n",
    "            mean_filtered_intercept = np.mean(filtered_intercept_list) if len(\n",
    "                filtered_intercept_list) > 0 else 0\n",
    "            calc_intercept = mean_filtered_intercept if self.intercept_list[\n",
    "                index_col] is None else self.intercept_list[index_col]\n",
    "\n",
    "            calc_y_ranked = calc_slope * X_col + calc_intercept\n",
    "\n",
    "            # remove duplicate values from self.y_train_ranked_transf and use indexes to remove items from self.y_train\n",
    "            y_train_ranked_transf_unique, sorted_indexes = np.unique(\n",
    "                self.y_train_ranked_transf, return_index=True)\n",
    "            y_train_unique = self.y_train[sorted_indexes]\n",
    "\n",
    "            try:\n",
    "                linear_interpolation = interpolate.interp1d(\n",
    "                    y_train_ranked_transf_unique, y_train_unique, fill_value=\"extrapolate\")\n",
    "                calc_y = linear_interpolation(calc_y_ranked)\n",
    "            except:\n",
    "                calc_y = np.mean(self.y_train)  \n",
    "            \n",
    "            pred_y_list.append(calc_y)\n",
    "        # Find the mean of the predicted y values\n",
    "        pred_y = np.mean(np.array(pred_y_list), axis=0)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "\n",
    "class MyPearsonRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, threshold=0.0):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        slope_list = []\n",
    "        intercept_list = []\n",
    "        for index_col in range(X.shape[1]):\n",
    "            X_col = X[:, index_col]\n",
    "            calc_slope, calc_intercept = self.find_model_params(X_col, y)\n",
    "            slope_list.append(calc_slope)\n",
    "            intercept_list.append(calc_intercept)\n",
    "        # Find the mean of the gradients and intercepts\n",
    "        self.slope_list = slope_list\n",
    "        self.intercept_list = intercept_list\n",
    "        return self\n",
    "\n",
    "    def find_model_params(self, X_col, y_col):\n",
    "        calc_cor = np.corrcoef(X_col, y_col)[0, 1]\n",
    "        # If the correlation is greater than the threshold, then calculate the gradient and intercept\n",
    "        if abs(calc_cor) > self.threshold:\n",
    "            calc_slope = calc_cor * np.std(y_col) / np.std(X_col)\n",
    "            calc_intercept = np.mean(y_col) - calc_slope * np.mean(X_col)\n",
    "        else:\n",
    "            calc_slope = None\n",
    "            calc_intercept = None\n",
    "        return calc_slope, calc_intercept\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_y_list = []\n",
    "        for index_col in range(X.shape[1]):\n",
    "            X_col = X[:, index_col]\n",
    "            # use the average of the slope_list as the default slope\n",
    "            filtered_slope_list = [x for x in self.slope_list if x is not None]\n",
    "            mean_filtered_slope = np.mean(filtered_slope_list) if len(\n",
    "                filtered_slope_list) > 0 else 0\n",
    "            calc_slope = mean_filtered_slope if self.slope_list[\n",
    "                index_col] is None else self.slope_list[index_col]\n",
    "\n",
    "            filtered_intercept_list = [\n",
    "                x for x in self.intercept_list if x is not None]\n",
    "            mean_filtered_intercept = np.mean(filtered_intercept_list) if len(\n",
    "                filtered_intercept_list) > 0 else 0\n",
    "            calc_intercept = mean_filtered_intercept if self.intercept_list[\n",
    "                index_col] is None else self.intercept_list[index_col]\n",
    "\n",
    "            calc_y = calc_slope * X_col + calc_intercept\n",
    "            pred_y_list.append(calc_y)\n",
    "        # Find the mean of the predicted y values\n",
    "        pred_y = np.mean(pred_y_list, axis=0)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "\n",
    "data_set_name = \"amgut1\"\n",
    "n_of_samples = 289\n",
    "index_of_pred_col = 0\n",
    "\n",
    "# Name some string contants\n",
    "out_dir = \"/scratch/da2343/cs685fall22/data\"\n",
    "out_file = out_dir + f'/my_algos_{str(date.today())}_results.csv'\n",
    "\n",
    "dataset_dict = {\n",
    "    \"amgut1\": \"/home/da2343/cs685_fall22/data/amgut1_data_standard_scaled.csv\",\n",
    "    # \"crohns\": \"/home/da2343/cs685_fall22/data/crohns_data_update.csv\",\n",
    "    # \"baxter_crc\": \"/home/da2343/cs685_fall22/data/baxter_crc_data_power_transformed.csv\",\n",
    "    # \"amgut2\": \"/home/da2343/cs685_fall22/data/amgut2_data_log_standard_scaled_transformed.csv\"\n",
    "}\n",
    "\n",
    "dataset_path = dataset_dict[data_set_name]\n",
    "n_splits = 3\n",
    "\n",
    "# Import the csv file of the dataset\n",
    "dataset_pd = pd.read_csv(dataset_path, header=0)\n",
    "sub_data_dict = {}\n",
    "output_vec = dataset_pd.iloc[:, index_of_pred_col].to_frame()\n",
    "input_mat = dataset_pd.drop(dataset_pd.columns[index_of_pred_col], axis=1)\n",
    "\n",
    "input_mat_update = input_mat.iloc[:n_of_samples].to_numpy()\n",
    "output_vec_update = output_vec.iloc[:n_of_samples].to_numpy().ravel()\n",
    "\n",
    "data_tuple = (input_mat_update,\n",
    "              output_vec_update, index_of_pred_col)\n",
    "\n",
    "# Create a list of alphas for the LASSOCV to cross-validate against\n",
    "# threshold_param_list = np.concatenate(\n",
    "#     (np.linspace(0, 0.2, 125), np.linspace(0.21, 0.4, 21), np.arange(0.5, 1.01, 0.1)))\n",
    "# threshold_param_dict = [{'threshold': [threshold]}\n",
    "#                         for threshold in threshold_param_list]\n",
    "\n",
    "threshold_param_list = np.concatenate(\n",
    "    (np.linspace(0, 0.4, 5), np.linspace(0.41, 0.6, 21), np.arange(0.7, 1.01, 0.1)))\n",
    "threshold_param_dict = [{'threshold': [threshold]}\n",
    "                        for threshold in threshold_param_list]\n",
    "\n",
    "learner_dict = {\n",
    "    # 'MultiVariateNormalModel' : MultiVariateNormalModel(),\n",
    "    # 'GuassianGraphicalMethod' : GuassianGraphicalMethod(),\n",
    "    # 'Pearson Correlation':  MyPearsonRegressor(),\n",
    "    'SpearmanRankRegressor': GridSearchCV(SpearmanRankRegressor(),\n",
    "                                threshold_param_dict,\n",
    "                                scoring='neg_mean_squared_error',\n",
    "                                return_train_score=True),\n",
    "    'Featureless': Featureless(),\n",
    "}\n",
    "\n",
    "\n",
    "test_err_list = []\n",
    "\n",
    "(input_mat, output_vec, index_col) = data_tuple\n",
    "k_fold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "for fold_id, indices in enumerate(k_fold.split(input_mat)):\n",
    "    index_dict = dict(zip([\"train\", \"test\"], indices))\n",
    "    set_data_dict = {}\n",
    "    for set_name, index_vec in index_dict.items():\n",
    "        set_data_dict[set_name] = {\n",
    "            \"X\": input_mat[index_vec],\n",
    "            \"y\": output_vec[index_vec]\n",
    "        }\n",
    "   \n",
    "    actual_y = set_data_dict[\"test\"][\"y\"]\n",
    "    x_col = set_data_dict[\"test\"][\"X\"][:, index_col]\n",
    "    \n",
    "    # Pearson model prediction\n",
    "    pearson = MyPearsonRegressor()\n",
    "    pearson.fit(**set_data_dict[\"train\"])\n",
    "    pearson_pred_y = pearson.predict(set_data_dict[\"test\"][\"X\"])\n",
    "    # Spearman model prediction   \n",
    "    spearman = GridSearchCV(SpearmanRankRegressor(),\n",
    "                                threshold_param_dict, \n",
    "                                return_train_score=True,\n",
    "                                )\n",
    "    spearman.fit(**set_data_dict[\"train\"])\n",
    "    # spearman.fit(**set_data_dict[\"train\"])\n",
    "    spearman_pred_y = spearman.best_estimator_.predict(set_data_dict[\"test\"][\"X\"])\n",
    "    # Featureless model prediction\n",
    "    featureless = Featureless()\n",
    "    featureless.fit(**set_data_dict[\"train\"])\n",
    "    featureless_pred_y = featureless.predict(set_data_dict[\"test\"][\"X\"])\n",
    "    \n",
    "    test_err_list.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"featureless_test_err2\": mean_squared_error(actual_y, featureless_pred_y),\n",
    "        \"spearman_test_err2\": mean_squared_error(actual_y, spearman_pred_y),\n",
    "        \"pearson_test_err2\": mean_squared_error(actual_y, pearson_pred_y),\n",
    "    })\n",
    "    \n",
    "\n",
    "test_err_df = pd.DataFrame(test_err_list)\n",
    "print(test_err_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   fold_id  featureless_test_err2  spearman_test_err2  pearson_test_err2\n",
    "0        0               0.174987            0.137678           0.206267\n",
    "1        1               0.197332            0.185014           0.205588\n",
    "2        2               1.835439            1.846116           1.838585"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
