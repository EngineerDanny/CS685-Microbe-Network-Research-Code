{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sys.argv)=11 so trying first param\n",
      "   Mean Squared Error  FoldID  # of Total Samples  Index of Subsample  \\\n",
      "0            1.307276       0                  10                   0   \n",
      "0            1.223761       0                  10                   0   \n",
      "0            0.989038       1                  10                   0   \n",
      "0            0.971309       1                  10                   0   \n",
      "0            0.463605       2                  10                   0   \n",
      "0            0.988257       2                  10                   0   \n",
      "0            0.583977       0                  10                   1   \n",
      "0            0.524363       0                  10                   1   \n",
      "0            0.146721       1                  10                   1   \n",
      "0            0.540605       1                  10                   1   \n",
      "0            2.217096       2                  10                   1   \n",
      "0            2.208465       2                  10                   1   \n",
      "0            1.830037       0                  10                   2   \n",
      "0            1.859626       0                  10                   2   \n",
      "0            0.356244       1                  10                   2   \n",
      "0            0.377458       1                  10                   2   \n",
      "0            1.026380       2                  10                   2   \n",
      "0            0.753496       2                  10                   2   \n",
      "0            0.847943       0                  10                   3   \n",
      "0            1.200228       0                  10                   3   \n",
      "0            0.916719       1                  10                   3   \n",
      "0            0.793540       1                  10                   3   \n",
      "0            2.447512       2                  10                   3   \n",
      "0            1.908523       2                  10                   3   \n",
      "0            0.547442       0                  10                   4   \n",
      "0            0.199832       0                  10                   4   \n",
      "0            3.250728       1                  10                   4   \n",
      "0            3.705986       1                  10                   4   \n",
      "0            1.341855       2                  10                   4   \n",
      "0            0.647996       2                  10                   4   \n",
      "0            0.474788       0                  10                   5   \n",
      "0            0.437370       0                  10                   5   \n",
      "0            1.362594       1                  10                   5   \n",
      "0            1.458143       1                  10                   5   \n",
      "0            1.381891       2                  10                   5   \n",
      "0            0.836931       2                  10                   5   \n",
      "\n",
      "              Dataset  Index of Predicted Column    Algorithm  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n",
      "0  necromass_bacteria                          0  Featureless  \n",
      "0  necromass_bacteria                          0     Spearman  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sys.path.append(os.path.abspath(\"/projects/genomic-ml/da2343/ml_project_1/shared\"))\n",
    "from model_header import *\n",
    "from constants import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "params_df = pd.read_csv(\"params.csv\")\n",
    "\n",
    "if len(sys.argv) == 2:\n",
    "    prog_name, task_str = sys.argv\n",
    "    param_row = int(task_str)\n",
    "else:\n",
    "    print(\"len(sys.argv)=%d so trying first param\" % len(sys.argv))\n",
    "    param_row = 0\n",
    "\n",
    "param_dict = dict(params_df.iloc[param_row, :])\n",
    "data_set_name = param_dict[\"Dataset\"]\n",
    "n_sub_samples = param_dict[\"# of Total Samples\"]\n",
    "index_of_pred_col = param_dict[\"Index of Prediction Col\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SpearmanRankRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, threshold=0.0):\n",
    "        self.threshold = threshold\n",
    "        self.preprocessor1 = make_pipeline(\n",
    "            MinMaxScaler(),\n",
    "            StandardScaler(),\n",
    "        )\n",
    "        self.preprocessor2 = make_pipeline(\n",
    "            MinMaxScaler(),\n",
    "            StandardScaler(),\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.y_train = y\n",
    "        # X_train_ranked_transf = ss.rankdata(X, axis=0)\n",
    "        self.y_train_ranked_transf = self.preprocessor2.fit_transform(\n",
    "            ss.rankdata(y).reshape(-1, 1)).flatten()\n",
    "        # self.y_train_ranked_transf = ss.rankdata(y)\n",
    "        X_train_ranked_transf = self.preprocessor1.fit_transform(\n",
    "            ss.rankdata(X, axis=0))\n",
    "        # X_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(X, axis=0))\n",
    "        # self.y_train_ranked_transf = PowerTransformer().fit_transform(ss.rankdata(y))\n",
    "\n",
    "        slope_list = []\n",
    "        intercept_list = []\n",
    "\n",
    "        for index_col in range(X_train_ranked_transf.shape[1]):\n",
    "            X_col = X_train_ranked_transf[:, index_col]\n",
    "            calc_slope, calc_intercept = self.find_model_params(\n",
    "                X_col, self.y_train_ranked_transf)\n",
    "            slope_list.append(calc_slope)\n",
    "            intercept_list.append(calc_intercept)\n",
    "        # Find the mean of the gradients and intercepts\n",
    "        self.slope_list = slope_list\n",
    "        self.intercept_list = intercept_list\n",
    "        return self\n",
    "\n",
    "    def find_model_params(self, X_col, y_col):\n",
    "        calc_cor = np.corrcoef(X_col, y_col)[0, 1]\n",
    "        # If the correlation is greater than the threshold, then calculate the gradient and intercept\n",
    "        if abs(calc_cor) > self.threshold:\n",
    "            calc_slope = calc_cor * np.std(y_col) / np.std(X_col)\n",
    "            calc_intercept = np.mean(y_col) - calc_slope * np.mean(X_col)\n",
    "        else:\n",
    "            calc_slope = None\n",
    "            calc_intercept = None\n",
    "        return calc_slope, calc_intercept\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred_y_list = []\n",
    "        X_test_ranked_transf = self.preprocessor1.fit_transform(\n",
    "            ss.rankdata(X, axis=0))\n",
    "        # X_test_ranked_transf = ss.rankdata(X, axis=0)\n",
    "\n",
    "        for index_col in range(X_test_ranked_transf.shape[1]):\n",
    "            X_col = X_test_ranked_transf[:, index_col]\n",
    "            # use the average of the slope_list as the default slope\n",
    "            filtered_slope_list = [x for x in self.slope_list if x is not None]\n",
    "            mean_filtered_slope = np.mean(filtered_slope_list) if len(\n",
    "                filtered_slope_list) > 0 else 0\n",
    "            calc_slope = mean_filtered_slope if self.slope_list[\n",
    "                index_col] is None else self.slope_list[index_col]\n",
    "\n",
    "            filtered_intercept_list = [\n",
    "                x for x in self.intercept_list if x is not None]\n",
    "            mean_filtered_intercept = np.mean(filtered_intercept_list) if len(\n",
    "                filtered_intercept_list) > 0 else 0\n",
    "            calc_intercept = mean_filtered_intercept if self.intercept_list[\n",
    "                index_col] is None else self.intercept_list[index_col]\n",
    "\n",
    "            calc_y_ranked = calc_slope * X_col + calc_intercept\n",
    "\n",
    "            # remove duplicate values from self.y_train_ranked_transf and use indexes to remove items from self.y_train\n",
    "            y_train_ranked_transf_unique, sorted_indexes = np.unique(\n",
    "                self.y_train_ranked_transf, return_index=True)\n",
    "            y_train_unique = self.y_train[sorted_indexes]\n",
    "\n",
    "            try:\n",
    "                linear_interpolation = interpolate.interp1d(\n",
    "                    y_train_ranked_transf_unique, y_train_unique, fill_value=\"extrapolate\")\n",
    "                calc_y = linear_interpolation(calc_y_ranked)\n",
    "                if np.isnan(calc_y).any():\n",
    "                   calc_y = [np.mean(self.y_train)] * len(calc_y_ranked)\n",
    "            except Exception as e:\n",
    "                calc_y = [np.mean(self.y_train)] * len(calc_y_ranked)\n",
    "                print(e)\n",
    "                \n",
    "            pred_y_list.append(calc_y)\n",
    "        # Find the mean of the predicted y values\n",
    "        pred_y = np.mean(np.array(pred_y_list), axis=0)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Name some string contants\n",
    "out_dir = \"/scratch/da2343/cs685fall22/data\"\n",
    "out_file = out_dir + f'/my_algos_{str(date.today())}_results.csv'\n",
    "\n",
    "dataset_path = dataset_dict[data_set_name]\n",
    "n_splits = 3\n",
    "# Import the csv file of the dataset\n",
    "dataset_pd = pd.read_csv(dataset_path, header=0)\n",
    "\n",
    "\n",
    "# Create a list of alphas for the LASSOCV to cross-validate against\n",
    "threshold_param_dict = [{'threshold': [threshold]}\n",
    "                        for threshold in np.concatenate((np.linspace(0, 0.4, 5), np.linspace(0.41, 0.6, 21), np.arange(0.7, 1.01, 0.1)))]\n",
    "learner_dict = {\n",
    "    \"Featureless\": Featureless(),\n",
    "    # 'Pearson':  MyPearsonRegressor(),\n",
    "    'Spearman':  SpearmanRankRegressor(),\n",
    "    # \"LASSO\": LassoCV(random_state=1),\n",
    "    # \"GGM\": GaussianGraphicalModel(),\n",
    "}\n",
    "\n",
    "\n",
    "test_err_list = []\n",
    "\n",
    "n_sections = int(np.floor(dataset_pd.shape[0] / n_sub_samples))\n",
    "shuffled_df = dataset_pd.sample(frac=1, random_state=1)\n",
    "total_samples = n_sub_samples * n_sections\n",
    "shuffled_df_updated = shuffled_df.iloc[:total_samples, :]\n",
    "shuffled_arr = np.split(shuffled_df_updated, n_sections)\n",
    "\n",
    "\n",
    "for ss_index, sub_section in enumerate(shuffled_arr):\n",
    "    # drop only one column per every iteration to form the input matrix\n",
    "    # make the column you removed the output\n",
    "    output_vec = sub_section.iloc[:, index_of_pred_col].to_numpy().ravel()  \n",
    "    input_mat = sub_section.drop(sub_section.columns[index_of_pred_col], axis=1).to_numpy()  \n",
    "    \n",
    "    k_fold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    for fold_id, indices in enumerate(k_fold.split(input_mat)):\n",
    "        index_dict = dict(zip([\"train\", \"test\"], indices))\n",
    "        set_data_dict = {}\n",
    "        for set_name, index_vec in index_dict.items():\n",
    "            set_data_dict[set_name] = {\n",
    "                \"X\": input_mat[index_vec],\n",
    "                \"y\": output_vec[index_vec]\n",
    "            }\n",
    "        # Loop through the learners\n",
    "        # Fit the learner to the training data\n",
    "        # Predict the test data\n",
    "        # Calculate the test error\n",
    "        for learner_name, learner in learner_dict.items():\n",
    "            learner.fit(**set_data_dict[\"train\"])\n",
    "            pred_y = learner.predict(set_data_dict[\"test\"][\"X\"])\n",
    "            actual_y = set_data_dict[\"test\"][\"y\"]\n",
    "            \n",
    "            mse = mean_squared_error(actual_y, pred_y)\n",
    "            # r2_coef = r2_score(actual_y, pred_y)\n",
    "\n",
    "            test_err_list.append(pd.DataFrame({\n",
    "                \"Mean Squared Error\": mse,\n",
    "                # \"Root Mean Squared Error\": np.sqrt(mse),\n",
    "                # \"R Squared\": pearsonr(actual_y, pred_y)[0] ** 2,\n",
    "                # \"R2 Score\": r2_coef,\n",
    "                \"FoldID\": fold_id,\n",
    "                \"# of Total Samples\": n_sub_samples,\n",
    "                \"Index of Subsample\": ss_index,\n",
    "                \"Dataset\": data_set_name,\n",
    "                \"Index of Predicted Column\": index_of_pred_col,\n",
    "                \"Algorithm\": learner_name,\n",
    "            }, index=[0]))\n",
    "\n",
    "main_test_err_df = pd.concat(test_err_list)\n",
    "print(main_test_err_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
